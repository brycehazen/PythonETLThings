import pandas as pd
import os

# specify the directory path where your csv files are located
path = 'C:/Users/bhoward/Desktop/OCA_Looking_For_duplicates/'


# create an empty list to store the dataframes
all_df = []

# create an empty set to store the ConsID
all_consid = set()

# create an empty dict to store the files and their ConsID
file_consid = {}

# create a counter variable to keep track of the total number of duplicates
counter = 0

# iterate through the files in the directory
for file in os.listdir(path):
    if file.endswith(".csv"):
        # read in the csv file
        df = pd.read_csv(path + file)
        # append the dataframe to the list
        all_df.append(df)
        # loop through the ConsID column
        for consid in df['ConsID']:
            if consid in all_consid:
                # if the consid is already in the set append the file name to the dict
                file_consid[consid].append(file)
                # increment the counter
                counter += 1
            else:
                # if the consid is not in the set add it to the set and add the file name to the dict
                all_consid.add(consid)
                file_consid[consid] = [file]

# concatenate all the dataframes into a single dataframe
combined_df = pd.concat(all_df)

# drop duplicates in the 'ConsID' column
combined_df = combined_df.drop_duplicates(subset='ConsID')

# check the count of rows
print(combined_df.shape[0])

# Create a new dataframe to store the duplicate ConsID
duplicate_df = pd.DataFrame(columns=['ConsID'])

# print the files and their shared ConsID
for key, value in file_consid.items():
    if len(value) > 1:
        print("ConsID: ", key)
        print("Files: ", value)
        print("\n")
        duplicate_df = duplicate_df.append({'ConsID': key}, ignore_index=True)

# print the total number of duplicates
print("Total number of duplicates: ", counter)

# save the duplicate ConsID to a csv file
duplicate_df.to_csv('duplicate_consid.csv', index=False)
